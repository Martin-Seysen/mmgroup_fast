/** @file mm_axis3_hadamard256.c

Yet to be documented

*/

#include <stdlib.h>
#include <stdint.h>
#include "mat24_functions.h"
#define MMGROUP_GENERATORS_INTERN
#include "mmgroup_generators.h"
#include "clifford12.h"
#include "mm_op_fast.h"
#include "mm_op_fast_intern.h"
#include "mm_op_sub.h"   


//  %%GEN h
/// @cond DO_NOT_DOCUMENT 
//  %%GEN c




/**********************************************************************
***********************************************************************
* Hadamard transform for an array of 256  byte (mod 128)
***********************************************************************
**********************************************************************/

/// @cond DO_NOT_DOCUMENT 

// When multiplying a vector of length 256 with a Hadamard matrix
// (modulo 128), we compute  a + (b ^ 0x7f) = a - b - 1 (mod 128)
// instead of a - b (mod 128) in many places. A final addition of
// the vector HADAMARD_DEFECT repairs this defect.  
static uint8_t ALIGNED(32) HADAMARD_DEFECT[256] = {
   // %%TABLE MM_TABLE_CASE2B_HADAMARD_DEFECT
};


/// @endcond 


/**********************************************************************
* Hadamard transform without GNU C vector extensions
**********************************************************************/

/// @cond DO_NOT_DOCUMENT 

#ifndef GCC_VECTORS

#define exch_u64_1_byte_pairs(a) \
  (((a >> 8) & 0xff00ff00ff00ffULL) | ((a & 0xff00ff00ff00ffULL) << 8))

#define exch_u64_2_byte_pairs(a) \
  (((a >> 16) & 0xffff0000ffffULL) | ((a & 0xffff0000ffffULL) << 16))

#define exch_u64_4_byte_pairs(a) \
  ((a >> 32) | (a << 32))

#if ENDIANESS != 0 && ENDIANESS != 1
#error Hadamard matrix in file mm_axis_case2B.c not implemented for unknown endianess
#endif

#if ENDIANESS == 0
#define HI_BYTE_MASK_1_U64 0x7f007f007f007f00ULL
#define HI_BYTE_MASK_2_U64 0x7f7f00007f7f0000ULL
#define HI_BYTE_MASK_4_U64 0x7f7f7f7f00000000ULL
#endif

#if ENDIANESS == 1
#define HI_BYTE_MASK_1_U64 0x007f007f007f007fULL
#define HI_BYTE_MASK_2_U64 0x00007f7f00007f7fULL
#define HI_BYTE_MASK_4_U64 0x000000007f7f7f7fULL
#endif

#define B7_MASK 0x7f7f7f7f7f7f7f7fULL


#define butterfly_b7(a, b) do { \
	register uint64_t _tmp = (a + (b ^ B7_MASK)) & B7_MASK; \
	a = (a + b) & B7_MASK; \
	b = _tmp;  \
} while (0)	
	
#define dbl_butterfly_b7(a, b, c, d) do { \
        register uint64_t _a = a, _b = b, _c = c, _d = d; \
	butterfly_b7(_a, _b); \
	butterfly_b7(_c, _d); \
	butterfly_b7(_a, _c); \
	butterfly_b7(_b, _d); \
        a = _a, b = _b, c = _c, d = _d; \
} while (0)	

/**  
  @brief Multiply bit vector of length 256 with Hadamard matrix

  Description see function ``hadamard256`` for the case with GNU C
  vector extensions.
*/
static inline void hadamard256(mmv_fast_row32_type *pa)
{
    uint32_t i;
    uint64_t *p0 = pa->u64, a, b;
    for (i = 0; i < 32; ++i) {
         a = p0[i] & B7_MASK;
         // %%FOR* k in (1,2,4)
         b = exch_u64_%{k}_byte_pairs(a);
         a ^= HI_BYTE_MASK_%{k}_U64;
         a = (a + b) & B7_MASK;
         // %%END FOR
         p0[i] = a;
    }
    for (i = 0; i < 32; i += 4) {
        dbl_butterfly_b7(p0[i], p0[i+1], p0[i+2], p0[i+3]); 
    }
    for (i = 0; i < 32; i += 8) {
        butterfly_b7(p0[i], p0[i+4]); 
        butterfly_b7(p0[i+1], p0[i+5]); 
        butterfly_b7(p0[i+2], p0[i+6]); 
        butterfly_b7(p0[i+3], p0[i+7]); 
    }
    for (i = 0; i < 8; i += 1) {
        dbl_butterfly_b7(p0[i], p0[i+8], p0[i+16], p0[i+24]); 
    }
    uint64_t *p_defect = (uint64_t*)HADAMARD_DEFECT;
    for (i = 0; i < 32; ++i) {
        p0[i] = (p0[i] + p_defect[i]) & B7_MASK;
    }
}

#endif //   ifndef GCC_VECTORS

/// @endcond 

/**********************************************************************
* Hadamard transform with GNU C vector extensions
**********************************************************************/

/// @cond DO_NOT_DOCUMENT 


#ifdef GCC_VECTORS
    #define F 0x7f
    static mmv_fast_row32_type ALIGNED(32) A_MASK = {
          {F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F}
    };
    #ifdef GCC_AVX2
        static mmv_fast_row32_type ALIGNED(32) A_SHUFFLE[4] = {
          {{ 1, 0, 3, 2, 5, 4, 7, 6, 9, 8,11,10,13,12,15,14,
            17,16,19,18,21,20,23,22,25,24,27,26,29,28,31,30}},
          {{ 2, 3, 0, 1, 6, 7, 4, 5,10,11, 8, 9,14,15,12,13,
            18,19,16,17,22,23,20,21,26,27,24,25,30,31,28,29}},
          {{ 4, 5, 6, 7, 0, 1, 2, 3,12,13,14,15, 8, 9,10,11,
            20,21,22,23,16,17,18,19,28,29,30,31,24,25,26,27}},
          {{ 8, 9,10,11,12,13,14,15, 0, 1, 2, 3, 4, 5, 6, 7,
            24,25,26,27,28,29,30,31,16,17,18,19,20,21,22,23}},
        };
        static mmv_fast_row32_type ALIGNED(32) A_NEG[5] = {
          {{0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F}},
          {{0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F}},
          {{0,0,0,0,F,F,F,F,0,0,0,0,F,F,F,F,0,0,0,0,F,F,F,F,0,0,0,0,F,F,F,F}},
          {{0,0,0,0,0,0,0,0,F,F,F,F,F,F,F,F,0,0,0,0,0,0,0,0,F,F,F,F,F,F,F,F}},
          {{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F}}
        };
    #else
        static v16_8_type ALIGNED(32) A_SHUFFLE[4] = {
          { 1, 0, 3, 2, 5, 4, 7, 6, 9, 8,11,10,13,12,15,14},
          { 2, 3, 0, 1, 6, 7, 4, 5,10,11, 8, 9,14,15,12,13},
          { 4, 5, 6, 7, 0, 1, 2, 3,12,13,14,15, 8, 9,10,11},
          { 8, 9,10,11,12,13,14,15, 0, 1, 2, 3, 4, 5, 6, 7},
        };
        static v16_8_type ALIGNED(16) A_NEG[4] = {
          {0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F},
          {0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F},
          {0,0,0,0,F,F,F,F,0,0,0,0,F,F,F,F},
          {0,0,0,0,0,0,0,0,F,F,F,F,F,F,F,F},
        };
    #endif   // #ifdef GCC_AVX2
    #undef F

/**  
  @brief Multiply bit vector of length 256 with Hadamard matrix

  Pointer ``pa->b`` points to a byte vector ``v`` of 256 integers of 8
  bit length. The function multiplies the vector ``v`` with a standard
  256 times 256 Hadamard matrix (with entries +1 and -1) in place.
  Entries of the  output vector ``v`` are reduced modulo 128.
*/
static inline void hadamard256(mmv_fast_row32_type *pa)
{
    uint32_t i;
    #ifdef GCC_AVX2
    for (i = 0; i < 8; ++i) {
        mmv_fast_row32_type ALIGNED(32) a = pa[i], b;
        a.v32[0] &= A_MASK.v32[0];
        // %%FOR* i in range(4)
        // Hadamard step %{int:1<<i}
        b.m256[0] = _mm256_shuffle_epi8(a.m256[0], A_SHUFFLE[%{i}].m256[0]);
        a.v32[0] ^= A_NEG[%{i}].v32[0];
        a.v32[0] = (a.v32[0] + b.v32[0]) &  A_MASK.v32[0];
        // %%END FOR
        // Hadamard step 16
        b.m256[0] = _mm256_permute4x64_epi64(a.m256[0], 0x4E);
        a.v32[0] ^= A_NEG[4].v32[0];
        a.v32[0] = (a.v32[0] + b.v32[0]) &  A_MASK.v32[0];
        pa[i] = a;
    }
    #else
    for (i = 0; i < 16; ++i) {
        v16_8_type ALIGNED(16) a, b;
        // %%FOR* i in range(4)
        // Hadamard step %{int:1<<i}
        a = pa->v16[i];
        b = __builtin_shuffle(a, A_SHUFFLE[%{i}]);
        a ^= A_NEG[%{i}];
        a = (a + b) &  A_MASK.v16[0];
        pa->v16[i] = a;
        // %%END FOR
    }
    // Hadamard step 16
    for (i = 0; i < 16; i += 2) {
        v16_8_type ALIGNED(16) a, b;
        a = pa->v16[i];
        b = pa->v16[i+1];
        pa->v16[i] = (a + b) & A_MASK.v16[0];
        pa->v16[i+1] = (a + (b ^ A_MASK.v16[0])) & A_MASK.v16[0];
    }
    #endif
    v32_8_type ALIGNED(32) a, b;
    // Hadamard step 16
    for (i = 0; i < 8; i += 2) {
        a = pa->v32[i];
        b = pa->v32[i+1];
        pa->v32[i] = (a + b) & A_MASK.v32[0];
        pa->v32[i+1] = (a + (b ^ A_MASK.v32[0])) & A_MASK.v32[0];
    }
    for (i = 0; i < 8; i += 4) {
        a = pa->v32[i];
        b = pa->v32[i+2];
        pa->v32[i] = (a + b) & A_MASK.v32[0];
        pa->v32[i+2] = (a + (b ^ A_MASK.v32[0])) & A_MASK.v32[0];
        a = pa->v32[i+1];
        b = pa->v32[i+3];
        pa->v32[i+1] = (a + b) & A_MASK.v32[0];
        pa->v32[i+3] = (a + (b ^ A_MASK.v32[0])) & A_MASK.v32[0];
    }
    for (i = 0; i < 4; i += 1) {
        a = pa->v32[i];
        b = pa->v32[i+4];
        pa->v32[i] = (a + b) & A_MASK.v32[0];
        pa->v32[i+4] = (a + (b ^ A_MASK.v32[0])) & A_MASK.v32[0];
    }
    v32_8_type *pA = (v32_8_type*)HADAMARD_DEFECT;
    for (i = 0; i < 8; i += 1) {
        pa->v32[i] = (pa->v32[i] + pA[i]) & A_MASK.v32[0];
    }
}




#endif //   ifdef GCC_VECTORS

/// @endcond 





/**********************************************************************
***********************************************************************
* Main function for transforming with Hadamard matrix
***********************************************************************
**********************************************************************/

/**  
  @brief Multiply bit vector of length 256 with Hadamard matrix

  Pointer ``pa->b`` points to a byte vector ``v`` of 256 integers of 8
  bit length. The function multiplies the vector ``v`` with a standard
  256 times 256 Hadamard matrix (with entries +1 and -1) in place.
  Entries of the  output vector ``v`` are reduced modulo 128.
*/
// %%EXPORT p
void mm_axis3_fast_hadamard256(mmv_fast_row32_type *pa)
{
    hadamard256(pa);
}




/**********************************************************************
* end of C functions
**********************************************************************/



//  %%GEN h
/// @endcond 
//  %%GEN c

