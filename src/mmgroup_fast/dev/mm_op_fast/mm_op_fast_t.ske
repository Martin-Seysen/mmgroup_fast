#define MM_OP_FAST_PERMUTATIONS
#define MM_OP_FAST_HADAMARD
#include <stdlib.h>
#include "mm_op_fast_types.h"
#include "mm_op_fast.h"
#include "mat24_functions.h"






//  %%GEN h
/// @cond DO_NOT_DOCUMENT 
//  %%GEN c




/***************************************************************************
** Constants for Hadamard part of  operation 't' on tag 'T'
***************************************************************************/


#ifdef GCC_VECTORS
  #ifdef SHUFFLE64
      static mmv_fast_row64_type ALIGNED(64) A_SHUFFLE[6] = {
        {{ 1, 0, 3, 2, 5, 4, 7, 6, 9, 8,11,10,13,12,15,14,
          17,16,19,18,21,20,23,22,25,24,27,26,29,28,31,30,
          33,32,35,34,37,36,39,38,41,40,43,42,45,44,47,46,
          49,48,51,50,53,52,55,54,57,56,59,58,61,60,63,62}},
        {{ 2, 3, 0, 1, 6, 7, 4, 5,10,11, 8, 9,14,15,12,13,
          18,19,16,17,22,23,20,21,26,27,24,25,30,31,28,29,
          34,35,32,33,38,39,36,37,42,43,40,41,46,47,44,45,
          50,51,48,49,54,55,52,53,58,59,56,57,62,63,60,61}},
        {{ 4, 5, 6, 7, 0, 1, 2, 3,12,13,14,15, 8, 9,10,11,
          20,21,22,23,16,17,18,19,28,29,30,31,24,25,26,27,
          36,37,38,39,32,33,34,35,44,45,46,47,40,41,42,43,
          52,53,54,55,48,49,50,51,60,61,62,63,56,57,58,59}},
        {{ 8, 9,10,11,12,13,14,15, 0, 1, 2, 3, 4, 5, 6, 7,
          24,25,26,27,28,29,30,31,16,17,18,19,20,21,22,23,
          40,41,42,43,44,45,46,47,32,33,34,35,36,37,38,39,
          56,57,58,59,60,61,62,63,48,49,50,51,52,53,54,55}},
        {{16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,
           0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,
          48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,
          32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47}},
        {{32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,
          48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,
           0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,
          16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31}}
      };
      #define F 0xff
      static mmv_fast_row64_type ALIGNED(64) A_NEG[6] = {
        {{0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,
          0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F}},
        {{0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,
          0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F}},
        {{0,0,0,0,F,F,F,F,0,0,0,0,F,F,F,F,0,0,0,0,F,F,F,F,0,0,0,0,F,F,F,F,
          0,0,0,0,F,F,F,F,0,0,0,0,F,F,F,F,0,0,0,0,F,F,F,F,0,0,0,0,F,F,F,F}},
        {{0,0,0,0,0,0,0,0,F,F,F,F,F,F,F,F,0,0,0,0,0,0,0,0,F,F,F,F,F,F,F,F,
          0,0,0,0,0,0,0,0,F,F,F,F,F,F,F,F,0,0,0,0,0,0,0,0,F,F,F,F,F,F,F,F}},
        {{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,
          0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F}},
        {{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
          F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F}}
      };
      #undef F
  #else
    #ifdef GCC_AVX2
        static mmv_fast_row32_type ALIGNED(32) A_SHUFFLE[4] = {
          {{ 1, 0, 3, 2, 5, 4, 7, 6, 9, 8,11,10,13,12,15,14,
            17,16,19,18,21,20,23,22,25,24,27,26,29,28,31,30}},
          {{ 2, 3, 0, 1, 6, 7, 4, 5,10,11, 8, 9,14,15,12,13,
            18,19,16,17,22,23,20,21,26,27,24,25,30,31,28,29}},
          {{ 4, 5, 6, 7, 0, 1, 2, 3,12,13,14,15, 8, 9,10,11,
            20,21,22,23,16,17,18,19,28,29,30,31,24,25,26,27}},
          {{ 8, 9,10,11,12,13,14,15, 0, 1, 2, 3, 4, 5, 6, 7,
            24,25,26,27,28,29,30,31,16,17,18,19,20,21,22,23}},
        };
        #define F 0xff
        static mmv_fast_row32_type ALIGNED(32) A_NEG[5] = {
          {{0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F}},
          {{0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F}},
          {{0,0,0,0,F,F,F,F,0,0,0,0,F,F,F,F,0,0,0,0,F,F,F,F,0,0,0,0,F,F,F,F}},
          {{0,0,0,0,0,0,0,0,F,F,F,F,F,F,F,F,0,0,0,0,0,0,0,0,F,F,F,F,F,F,F,F}},
          {{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F,F}}
        };
        #undef F
    #else
        static v16_8_type ALIGNED(32) A_SHUFFLE[4] = {
          { 1, 0, 3, 2, 5, 4, 7, 6, 9, 8,11,10,13,12,15,14},
          { 2, 3, 0, 1, 6, 7, 4, 5,10,11, 8, 9,14,15,12,13},
          { 4, 5, 6, 7, 0, 1, 2, 3,12,13,14,15, 8, 9,10,11},
          { 8, 9,10,11,12,13,14,15, 0, 1, 2, 3, 4, 5, 6, 7},
        };
        #define F 0xff
        static v16_8_type ALIGNED(16) A_NEG[4] = {
          {0,F,0,F,0,F,0,F,0,F,0,F,0,F,0,F},
          {0,0,F,F,0,0,F,F,0,0,F,F,0,0,F,F},
          {0,0,0,0,F,F,F,F,0,0,0,0,F,F,F,F},
          {0,0,0,0,0,0,0,0,F,F,F,F,F,F,F,F},
        };
        #undef F
    #endif   // #ifdef GCC_AVX2
  #endif   // #ifdef SHUFFLE64
#endif   // #ifdef GCC_VECTORS





/***************************************************************************
**  Operation 't' on tags 'ABC'
***************************************************************************/



static inline void ALWAYS_INLINE
op_t_ABC_mod_3(mmv_fast_row32_type *p_in, uint32_t t1, mmv_fast_row32_type *p_out)
{
    uint32_t i;
    ASSUME_ALIGNED(p_in, 32);
    ASSUME_ALIGNED(p_out, 32);
    mmv_fast_row32_type ALIGNED(32) sum, diff, r0, r1;
    uint8_t *pd_in, *pd_out;
    for (i = 0; i < 24; ++i) {
        // put sum = (b+c)/2, diff = (b-c)/2
        mmv_row24_neg(p_in[i + 24], &r0);
        r1 = p_in[i + 48];
        hadamard_row32_add_mod3(r0, r1, &diff);
        mmv_row24_neg(r1, &r1);
        hadamard_row32_add_mod3(r0, r1, &sum);
        r0 = p_in[i];
        if (t1) {
            p_out[i] = diff;
            hadamard_row32_add_mod3(r0, sum, &p_out[i + 24]);
            mmv_row24_neg(sum, &sum);
            hadamard_row32_add_mod3(r0, sum, &p_out[i + 48]);
       } else {
            p_out[i] = sum;
            hadamard_row32_add_mod3(r0, diff, &p_out[i + 24]);
            mmv_row24_neg(r0, &r0);
            hadamard_row32_add_mod3(r0, diff, &p_out[i + 48]);
      }
    }
    pd_in = (uint8_t*)(p_in);
    pd_out = (uint8_t*)(p_out);
    for (i = 0; i < 24*32; i += 33) pd_out[i] = pd_in[i]; 
}




/***************************************************************************
** Hadamard part of  operation 't' on tag 'T'
***************************************************************************/



static inline void ALWAYS_INLINE
hadamard_row64_mod_3(mmv_fast_row64_type *r)
// Multiply a 64-bit vector r0[0] with a 64 x 64 hadamard matrix. Here r0
// must be an array of three 64-bit vectors of type mmv_fast_row64_type.
// The last two entrirs of that array are used as work buffer.
{
  #define rr r[0]
  #define aa r[1] 
  #define bb r[2]
  // DUMP("hadamard_row64_mod_3 r=%p", r); 
  #ifdef GCC_VECTORS
    #ifdef SHUFFLE64
      //v64_8_type ALIGNED(64) a, b;
      // %%FOR* i in range(6)
      aa.v64[0] = rr.v64[0];
      _shuffle64(aa, A_SHUFFLE[%{i}], &bb);
      aa.v64[0] ^= A_NEG[%{i}].v64[0];
      hadamard_row64_add_mod3(aa, bb, &rr);
      // %%END FOR
      rr.v64[0] ^= (uint8_t)0xff;
    #else
      #ifdef GCC_AVX2
        // mmv_fast_row32_type ALIGNED(32) a, b;
        // %%FOR* i in range(4)
        // Hadamard step %{int:1<<i}
        // %%FOR* j in range(2)
        aa.v32[0] = rr.v32[%{j}];
        bb.m256[0] = _mm256_shuffle_epi8(aa.m256[0], A_SHUFFLE[%{i}].m256[0]);
        aa.v32[0] ^= A_NEG[%{i}].v32[0];
        hadamard_row32_add_mod3(aa.v32row[0], bb.v32row[0], &rr.v32row[%{j}]);
        // %%END FOR
        // %%END FOR
        // Hadamard step 16
        // %%FOR* j in range(2)
        aa.v32[0] = rr.v32[%{j}];
        bb.m256[0] = _mm256_permute4x64_epi64(aa.m256[0], 0x4E);
        aa.v32[0] ^= A_NEG[4].v32[0];
        hadamard_row32_add_mod3(aa.v32row[0], bb.v32row[0], &rr.v32row[%{j}]);
        // %%END FOR
        // Hadamard step 32 (divide entry by 8 in last step)
        aa.v32[0] = rr.v32[0] ^ 0xff;
        bb.v32[0] = rr.v32[1];
        hadamard_row32_add_mod3(aa.v32row[0], bb.v32row[0], &rr.v32row[1]);
        bb.v32[0] ^= 0xff;
        hadamard_row32_add_mod3(aa.v32row[0], bb.v32row[0], &rr.v32row[0]);
      #else
        static v16_8_type ALIGNED(16) HI_MASK = {
          0xaa,0xaa,0xaa,0xaa,0xaa,0xaa,0xaa,0xaa,
          0xaa,0xaa,0xaa,0xaa,0xaa,0xaa,0xaa,0xaa
        };
        v16_8_type ALIGNED(16) a, b;
        // %%FOR* i in range(4)
        // Hadamard step %{int:1<<i}
        // %%FOR* j in range(4)
        a = rr.v16[%{j}];
        b = __builtin_shuffle(a, A_SHUFFLE[%{i}]);
        a ^= A_NEG[%{i}];
        add_mod_3(rr.v16[%{j}], a, b, HI_MASK)
        // %%END FOR
        // %%END FOR
        // Hadamard step 16
        // %%FOR* j in [0, 2]
        a = rr.v16[%{j}];
        b = rr.v16[%{int:j+1}];
        add_mod_3(rr.v16[%{j}], a, b, HI_MASK)
        b ^= 0xff;
        add_mod_3(rr.v16[%{int:j+1}], a, b, HI_MASK)
        // %%END FOR
        // Hadamard step 32 (divide entry by 8 in last step)
        // %%FOR* j in [0, 1]
        a = rr.v16[%{j}] ^ 0xff;
        b = rr.v16[%{int:j+2}];
        add_mod_3(rr.v16[%{int:j+2}], a, b, HI_MASK)
        b ^= 0xff;
        add_mod_3(rr.v16[%{j}], a, b, HI_MASK)
        // %%END FOR
      #endif   // #ifdef GCC_AVX2
    #endif   // #ifdef SHUFFLE64
  #else
    #ifndef ENDIANESS
    #error Function hadamard_row64_type_mod_3() not implemented for unknown endianess
    #endif
    uint64_t t0, t1, t2;
    // %%FOR* sh in [1,2,4]
    // Hadamard step %{sh}
    // %%FOR* i in range(8)
    t0 = rr.u64[%{i}];  
    t1 = exch_u64_%{sh}_byte_pairs(t0);
    t0 ^= HI_BYTE_MASK_%{sh}_U64;
    add_mod_3_u64(t2, t0, t1)
    rr.u64[%{i}] = t2;
    // %%END FOR
    // %%END FOR
    // %%FOR* d, j_list in [(1,[0,2,4,6]), (2,[0,1,4,5])]
    // Hadamard step %{int:8 * d}
    // %%FOR* j in j_list
    t0 = rr.u64[%{j}]; t1 = rr.u64[%{int:j+d}]; 
    add_mod_3_u64(t2, t0, t1)
    rr.u64[%{j}] = t2;
    t1 = ~t1;
    add_mod_3_u64(t2, t0, t1)
    rr.u64[%{int:j+d}] = t2;
    // %%END FOR
    // %%END FOR
    // Hadamard step 32; (divide entry by 8 in last step)
    // %%FOR* j in range(4)
    t0 = ~rr.u64[%{j}]; t1 = rr.u64[%{int:j+4}]; 
    add_mod_3_u64(t2, t0, t1)
    rr.u64[%{int:j+4}] = t2;
    t1 = ~t1;
    add_mod_3_u64(t2, t0, t1)
    rr.u64[%{j}] = t2;
    // %%END FOR
  #endif   // #ifdef GCC_VECTORS
  #undef rr
  #undef aa 
  #undef bb 
}


static inline void ALWAYS_INLINE
hadamard_row64_swap_parities(mmv_fast_row64_type *a)
{
  #ifdef GCC_VECTORS
    #ifdef SHUFFLE64
      static mmv_fast_row64_type ALIGNED(64) A_SHUFFLE = {
        { 0,62,61, 3,59, 5, 6,56,55, 9,10,52,12,50,49,15,
         47,17,18,44,20,42,41,23,24,38,37,27,35,29,30,32,
         31,33,34,28,36,26,25,39,40,22,21,43,19,45,46,16,
         48,14,13,51,11,53,54, 8, 7,57,58, 4,60, 2, 1,63}
      };
      _shuffle64(*a, A_SHUFFLE, a);
    #else
      #ifdef GCC_AVX2
        static mmv_fast_row64_type ALIGNED(64) A_SHUFFLE = {
          { 0,62,61, 3,59, 5, 6,56,55, 9,10,52,12,50,49,15,
           47,17,18,44,20,42,41,23,24,38,37,27,35,29,30,32,
           31,33,34,28,36,26,25,39,40,22,21,43,19,45,46,16,
           48,14,13,51,11,53,54, 8, 7,57,58, 4,60, 2, 1,63}
        };
        #define F 0xff
        static mmv_fast_row64_type ALIGNED(64) A_BLEND = {
           {0,F,F,0,F,0,0,F, F,0,0,F,0,F,F,0, F,0,0,F,0,F,F,0, 0,F,F,0,F,0,0,F}
        };
        #undef F 
        __m256i t0, t1;
        a->m256[0] = _mm256_shuffle_epi8(a->m256[0], A_SHUFFLE.m256[0]);
        a->m256[1] = _mm256_shuffle_epi8(a->m256[1], A_SHUFFLE.m256[1]);
        t0 = _mm256_permute4x64_epi64(a->m256[1], 0x4E);
        t1 = _mm256_permute4x64_epi64(a->m256[0], 0x4E);
        a->m256[0] = _mm256_blendv_epi8(a->m256[0], t0, A_BLEND.m256[0]);
        a->m256[1] = _mm256_blendv_epi8(t1, a->m256[1], A_BLEND.m256[0]);
      #else
        static v16_8_type A_SHUFFLE[4] ALIGNED(16) = {
          { 0,30,29, 3,27, 5, 6,24,  23, 9,10,20,12,18,17,15},
          {31, 1, 2,28, 4,26,25, 7,   8,22,21,11,19,13,14,16},
          {15,17,18,12,20,10, 9,23,  24, 6, 5,27, 3,29,30, 0},
          {16,14,13,19,11,21,22, 8,   7,25,26, 4,28, 2, 1,31}
        };
        v16_8_type t0, t1;
        t0 = a->v16[0];
        t1 = a->v16[3];
        a->v16[0] = __builtin_shuffle(t0, t1, A_SHUFFLE[0]);
        a->v16[3] = __builtin_shuffle(t0, t1, A_SHUFFLE[3]);
        t0 = a->v16[1];
        t1 = a->v16[2];
        a->v16[1] = __builtin_shuffle(t0, t1, A_SHUFFLE[1]);
        a->v16[2] = __builtin_shuffle(t0, t1, A_SHUFFLE[2]);
      #endif
    #endif
  #else
    #ifndef ENDIANESS
    #error Function hadamard_row64_swap_parities() not implemented for unknown endianess
    #endif
    mmv_fast_row64_type r = *a;
    // %%FOR* i in range(64)
    // %%IF* bitweight6(i) & 1
    a->b[%{i}] = r.b[%{int:63-i}];
    // %%END IF
    // %%END FOR
  #endif
}



/***************************************************************************
** Test operation 't' on tag 'T'
***************************************************************************/


// %%EXPORT px
int32_t mm_op_fast_test_op_t_tag_T(uint32_t p, uint32_t mode, uint8_t *data)
{
    mmv_fast_row64_type *r = (mmv_fast_row64_type*)alloca_aligned(3*64, 64);
    ASSUME_ALIGNED(r, 64);
    memcpy(r, data, 64);
    DUMP("TTT 1 mode=%d, p=%p", mode, r);
    if (mode & 1) hadamard_row64_neg_parities(r, 1);
    DUMP("TTT 2", 0);
    if (mode & 2) hadamard_row64_swap_parities(r);
    DUMP("TTT 3", 0);
    if (mode & 4) switch (p) {
        case 3:
            hadamard_row64_mod_3(r);
            break;
        default:
            return -1;  
    }
    DUMP("TTT 4", 0);
    if (mode & 8) hadamard_row64_neg_parities(r, 1);
    memcpy(data, r, 64);
    return 0;
}





/***************************************************************************
** Functions performing the permutation on a vector of bytes
***************************************************************************/



/// @cond DO_NOT_DOCUMENT 


static int32_t mm_op_fast_op_t_mma0(
    uint32_t p, uint32_t t, mmv_fast_type *p_in, mmv_fast_type *p_out
)
{
    ASSUME_ALIGNED(p_in, 64);
    ASSUME_ALIGNED(p_out, 64);
    mmv_fast_row64_type r[3];
    DUMP("m_op_fast_op_t_mma0 r=%p, t=%d", r, t);
    uint32_t t1, i;
    static uint32_t src[3] = {
        MM_AUX_OFS_X/32, MM_AUX_OFS_Y/32, MM_AUX_OFS_Z/32
    };
    static uint32_t dest[2][3] = {
        {MM_AUX_OFS_Y/32, MM_AUX_OFS_Z/32, MM_AUX_OFS_X/32},
        {MM_AUX_OFS_Z/32, MM_AUX_OFS_X/32, MM_AUX_OFS_Y/32},
    };
    static uint8_t mode[2][3] = {
       {1,3,2}, {2,1,3}
    };
    // DUMP("op t start  %p %p", p_in, p_out);

    t &= 3;
    if ((t - 1) & 2) {
        memcpy(p_out, p_in, sizeof(mmv_fast_type));
        return 0;
    } 
    t1 = t - 1;

    {
        // Do tags 'ABC' here
        mmv_fast_row32_type *pi_in, *pi_out;
        pi_in = (mmv_fast_row32_type *)p_in;
        pi_out = (mmv_fast_row32_type *)p_out;
        ASSUME_ALIGNED(pi_in, 32);
        ASSUME_ALIGNED(pi_out, 32);
        switch (p) {
            case 3:
                op_t_ABC_mod_3(pi_in, t1, pi_out);
                break;
            default:
                return -1;  
        }
    }

    {   
        // Do tag 'T' here
        uint32_t i;
        mmv_fast_row64_type *r_in = p_in->r64 + MM_AUX_OFS_T/64;
        ASSUME_ALIGNED(r_in, 64);
        mmv_fast_row64_type *r_out = p_out->r64 + MM_AUX_OFS_T/64;
        ASSUME_ALIGNED(r_out, 64);
        DUMP("op T %p", &r);
        for (i = 0; i < 759; ++i) {
            *r = r_in[i];
            DUMP_COND(i < 3, "op T i=%d, neg", i);
            hadamard_row64_neg_parities(r, t1);
            DUMP_COND(i < 3, "op T i=%d, swap", i);
            hadamard_row64_swap_parities(r);
            switch (p) {
                case 3:
                    DUMP_COND(i < 2, "op T Hadamard %p", r);
                    hadamard_row64_mod_3(r);
                    break;
                default:
                    return -1;  
            }
            hadamard_row64_neg_parities(r, 1 - t1);
            r_out[i] = *r;
        }   
    }

    for (i = 0; i < 3; ++i)  mm_op_fast_neg_op_t_mma0(
        (mmv_fast_row32_type *)(p_in->v32 + src[i]),
        (mmv_fast_row32_type *)(p_out->v32 + dest[t1][i]),
        mode[t1][i]
    );

    return 0;
}

/// @endcond


/***************************************************************************
** Main function performing the permutation on a vector
***************************************************************************/



// %%EXPORT p
int32_t mm_op_fast_op_t(mmv_fast_matrix_type *pm, uint32_t t)
{
    uint32_t cur = pm->current &= 1;
    switch (pm->mode) {
        case 1:
            {
                mmv_fast_type *p_in = pm->p_v.p_vb[cur];  
                mmv_fast_type *p_out = pm->p_v.p_vb[cur ^ 1];  
                t &= 3;
                if ((t - 1) & 2) return 0;
                pm->current = cur ^ 1;  
                return mm_op_fast_op_t_mma0(pm->p, t, p_in, p_out);
            }
        default:
            return -1;   
    }
}




//  %%GEN h
/// @endcond 
//  %%GEN c

